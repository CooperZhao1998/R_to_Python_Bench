# r2py-bench (mini)
A tiny, reproducible benchmark to test AI's ability to convert **R (tidyverse)** to **Python (Pandas/Polars)** for dataframe manipulation.

## Folder layout
- `data/` — synthetic sample datasets (CSV for portability here)
- `tasks/` — 10 R scripts; each writes `expected/NNN.parquet`
- `expected/` — ground-truth outputs (generated by running R tasks)
- `ai_outputs/` — put AI-translated Python files here as `NNN.py` (must define `out`)
- `harness/` — grader + prompt template

## Quick start
1) In R, run:
   ```r
   setwd("r2py-bench")
   source("harness/r_make_expected.R")
   ```
   This writes `expected/*.parquet` using Arrow.

2) Ask your AI to convert each R task using the template in `harness/ai_prompt_template.txt`.
   Save each answer as `ai_outputs/NNN.py` (Pandas or Polars OK).

3) In Python, grade:
   ```bash
   cd r2py-bench
   pip install pandas pyarrow numpy fastparquet
   python harness/grade_python.py
   ```

## Scoring
- Pass/Fail per task based on column order, row order, NA placement, and values (with tiny float tolerance).
- Overall Score = % tasks passed.

## Notes
- Semi/anti joins, case_when, timezones, ordered categories, and list-cols are included to expose subtle bugs.
- You can extend `tasks/` with your own R code; just ensure it writes a single `expected/NNN.parquet`.
